<!DOCTYPE html>
<html lang="zh-tw">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>ERANN | Siuoly&#39;s site</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="description" content="https://paperswithcode.com/sota/audio-classification-on-esc-50 [TOC]
Paper Do  Propose convolutional neural network frameworks improving the inference speed of CNN-based systems. Investigate the impact of (1)data augmentation techniques ,and (2) transfer learning.  Audio pattern recognition (APR )  Environmental sound classification(環境聲音分類) Sound event detection(聲音事件檢測) Audio tagging(音頻標記) Smart room monitoring(智能房間監控) Video content highlight generation(視頻內容突出顯示生成) Musical genre classification(音樂類型分類) Speech emotion classification(語音情感分類) Classify respiratory diseases(分類呼吸系統疾病)  Related work Traditional Methods Traditional APR systems are classical generative or discriminative models, e.">
    <meta name="generator" content="Hugo 0.95.0" />
    
    
    
    
      <meta name="robots" content="noindex, nofollow">
    

    
<link rel="stylesheet" href="/ananke/css/main.min.css" >



    
    
    
      

    

    
    
    <meta property="og:title" content="ERANN" />
<meta property="og:description" content="https://paperswithcode.com/sota/audio-classification-on-esc-50 [TOC]
Paper Do  Propose convolutional neural network frameworks improving the inference speed of CNN-based systems. Investigate the impact of (1)data augmentation techniques ,and (2) transfer learning.  Audio pattern recognition (APR )  Environmental sound classification(環境聲音分類) Sound event detection(聲音事件檢測) Audio tagging(音頻標記) Smart room monitoring(智能房間監控) Video content highlight generation(視頻內容突出顯示生成) Musical genre classification(音樂類型分類) Speech emotion classification(語音情感分類) Classify respiratory diseases(分類呼吸系統疾病)  Related work Traditional Methods Traditional APR systems are classical generative or discriminative models, e." />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://siuoly.github.io/posts/erann/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-03-24T17:05:40+08:00" />
<meta property="article:modified_time" content="2022-03-24T17:05:40+08:00" />

<meta itemprop="name" content="ERANN">
<meta itemprop="description" content="https://paperswithcode.com/sota/audio-classification-on-esc-50 [TOC]
Paper Do  Propose convolutional neural network frameworks improving the inference speed of CNN-based systems. Investigate the impact of (1)data augmentation techniques ,and (2) transfer learning.  Audio pattern recognition (APR )  Environmental sound classification(環境聲音分類) Sound event detection(聲音事件檢測) Audio tagging(音頻標記) Smart room monitoring(智能房間監控) Video content highlight generation(視頻內容突出顯示生成) Musical genre classification(音樂類型分類) Speech emotion classification(語音情感分類) Classify respiratory diseases(分類呼吸系統疾病)  Related work Traditional Methods Traditional APR systems are classical generative or discriminative models, e."><meta itemprop="datePublished" content="2022-03-24T17:05:40+08:00" />
<meta itemprop="dateModified" content="2022-03-24T17:05:40+08:00" />
<meta itemprop="wordCount" content="1211">
<meta itemprop="keywords" content="" /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="ERANN"/>
<meta name="twitter:description" content="https://paperswithcode.com/sota/audio-classification-on-esc-50 [TOC]
Paper Do  Propose convolutional neural network frameworks improving the inference speed of CNN-based systems. Investigate the impact of (1)data augmentation techniques ,and (2) transfer learning.  Audio pattern recognition (APR )  Environmental sound classification(環境聲音分類) Sound event detection(聲音事件檢測) Audio tagging(音頻標記) Smart room monitoring(智能房間監控) Video content highlight generation(視頻內容突出顯示生成) Musical genre classification(音樂類型分類) Speech emotion classification(語音情感分類) Classify respiratory diseases(分類呼吸系統疾病)  Related work Traditional Methods Traditional APR systems are classical generative or discriminative models, e."/>

	
  </head>

  <body class="ma0 avenir bg-near-white">

    
   
  

  <header>
    <div class="bg-black">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l justify-between items-center center">
    <a href="/" class="f3 fw2 hover-white no-underline white-90 dib">
      
        Siuoly&#39;s site
      
    </a>
    <div class="flex-l items-center">
      

      
      
<div class="ananke-socials">
  
</div>
    </div>
  </div>
</nav>

    </div>
  </header>



    <main class="pb7" role="main">
      
  
  <article class="flex-l flex-wrap justify-between mw8 center ph3">
    <header class="mt4 w-100">
      <aside class="instapaper_ignoref b helvetica tracked">
          
        POSTS
      </aside>
      










  <div id="sharing" class="mt3 ananke-socials">
    
  </div>


      <h1 class="f1 athelas mt3 mb1">ERANN</h1>
      
      
      
      <time class="f6 mv4 dib tracked" datetime="2022-03-24T17:05:40+08:00">March 24, 2022</time>
      

      
      
    </header>
    <div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l w-two-thirds-l"><p><a href="https://paperswithcode.com/sota/audio-classification-on-esc-50">https://paperswithcode.com/sota/audio-classification-on-esc-50</a>
<img src="img/title.png" alt="title"></p>
<!-- raw HTML omitted -->
<p>[TOC]</p>
<!-- raw HTML omitted -->
<h2 id="paper-do">Paper Do</h2>
<ol>
<li>Propose convolutional neural network frameworks improving the inference speed of CNN-based systems.</li>
<li>Investigate the impact of <strong>(1)data augmentation techniques</strong> ,and <strong>(2) transfer learning</strong>.</li>
</ol>
<h2 id="audio-pattern-recognition-apr-">Audio pattern recognition (APR )</h2>
<ul>
<li>Environmental sound classification(環境聲音分類)</li>
<li>Sound event detection(聲音事件檢測)</li>
<li>Audio tagging(音頻標記)</li>
<li>Smart room monitoring(智能房間監控)</li>
<li>Video content highlight generation(視頻內容突出顯示生成)</li>
<li>Musical genre classification(音樂類型分類)</li>
<li>Speech emotion classification(語音情感分類)</li>
<li>Classify respiratory diseases(分類呼吸系統疾病)</li>
</ul>
<h2 id="related-work">Related work</h2>
<h3 id="traditional-methods">Traditional Methods</h3>
<p>Traditional APR systems are classical generative or discriminative models, e.g., (1)<strong>Gaussian mixture models(GMMs)</strong>, and used (2)time-frequency representations: <strong>log mel spectrogram</strong> or <strong>mel-frequency cepstral coefficients (MFCCs)</strong> as input.<br>
Methods with neural networks, in particular with <strong>CNNs</strong>, significantly <strong>outperformed conservative machine learning</strong> methods for APR tasks. <br>
<strong>CNN-based</strong> systems <strong>outperformed</strong> recurrent neural network <strong>(RNN)-based</strong> systems.</p>
<!-- raw HTML omitted -->
<blockquote>
<p>(GMMs)“An MFCC-GMM approach for event detection and classification,” in IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA), 2013.\<br>
(CNNs)“Learning from Between-class Examples for Deep Sound Recognition,” in International Conference on Learning Representations, 2018.  <br>
(CNNs)“PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition,” IEEE/ACM Transactions on Audio, Speech, and Language Processing,  2020. (CNNs) <br>
(CNNs vs. RNNs)“A comparison of Deep Learning methods for environmental sound detection,” in 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2017  ,</p>
</blockquote>
<h3 id="transformer-based-systems">Transformer-based systems</h3>
<p>A <strong>transformer-based</strong> system achieves the state-of-the-art performance with an <strong>mAP of 0.485</strong> on the AudioSet dataset.<br>
However, <strong>high computational complexity</strong> (526.6 million parameters), which makes it difficult to apply this system in real life.</p>
<blockquote>
<p>Y. Gong, Y.-A. Chung, and J. Glass, “AST: Audio Spectrogram Transformer, 2021, MIT Computer Science and Artificial Intelligence Laboratory, Cambridge”</p>
</blockquote>
<h3 id="cnn-based-systems">CNN-based systems</h3>
<ol>
<li>Use models with <strong>2D convolutional layers</strong> and <strong>time–frequency representations</strong>, e.g., the log <strong>mel spectrogram</strong>, as <strong>input</strong> to the first 2D convolutional layer.</li>
<li><strong>End-to-end systems</strong>, where the raw audio signal is used as input. comprise two parts.
<ol>
<li>First part includes <strong>1D convolutional layers</strong>, applied to extract 2D features, which replace time– frequency representations.</li>
<li>Fedd features from previous step to 2D convolutional layers.</li>
</ol>
</li>
</ol>
<p>Systems with the <strong>log mel spectrogram</strong> perform <strong>better</strong> on APR tasks.<br>
Moreover, there are end-to-end systems, which contain only 1D convolutional layers .</p>
<h3 id="transfer-learning">Transfer learning</h3>
<p>A model is trained for <strong>tasks with a large dataset</strong> and transferred to <strong>similar tasks with smaller datasets</strong>: all parameters of the <strong>model</strong> for the new task are <strong>initialized from the pre-trained model</strong>, except parameters of few last layers.</p>
<p>In this article, we use <strong>fine-tuning</strong> as the transfer learning strategy (we <strong>optimize all parameters</strong> of transferred models)</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<h3 id="residual-neural-network殘差神經網絡">Residual neural network(殘差神經網絡)</h3>
<p><strong>Residual neural networks (ResNets)</strong> have <strong>shortcut connections</strong> among convolutional layers, which help partially avoid the <strong>vanishing gradient</strong> problem.</p>
<p><strong>WideResNets</strong> have an additional hyperparameter—the <strong>widening factor(擴大因子)</strong> for the width of convolutional layers—to change the <strong>computational complexity</strong> of models. With optimal values of the widening factor, WideResNets have <strong>better performance</strong> and <strong>fewer parameters</strong> than original ResNets.</p>
<p>This paper use <strong>shortcut connections</strong> <strong>and</strong> the <strong>widening factor</strong> in CNN architecture.</p>
<h2 id="methods">Methods</h2>
<h4 id="hyperparameters">Hyperparameters</h4>
<ul>
<li>Log-Mel spectrogram
<ul>
<li><strong>sampling rate</strong>: 44.1k Hz</li>
<li>(STFT) with the Hann <strong>window of size</strong>: 1380 (≈ 31 ms)</li>
<li><strong>hop size</strong> : 345 (≈ 8 ms)</li>
<li>$T_s = 128t$, where $T_s$ is number of time frame, and $t$ is the time duration(sec).</li>
<li><strong>Mel bin</strong> : 128,  which is the best choice as a trade-off between <strong>computational complexity</strong> and <strong>performance</strong> of models.</li>
<li>$f_{min} = 50 Hz$ ,   lower cut-off frequency (remove low-frequency noise)</li>
<li>$f_{max} = 14000 Hz$,  upper cut-off frequency  (remove the aliasing effect)</li>
</ul>
</li>
</ul>
<h4 id="eranns-architecture">ERANNs Architecture</h4>
<p>Log mel spectrogram is 2D tensor with a shape of 128 ×$T_s$.<br>
Each tensor between CNN blocks has a shape of $F_i ×T_i ×C_i$, where $F_i$ is “the i-th frequency size”, $T_i$ is “the ith temporal size” and $C_i$ is the number of channels.<br>
$s_m$ is the <strong>decreasing temporal size parameter.</strong> <strong>Temporal sizes $T_i$ are reduced stronger</strong> based on <strong>increased stride sizes</strong> of convolutional layers.<br>
$W$ is the widening factor and $s_i$ are stride sizes.</p>
<!-- raw HTML omitted -->
<h4 id="audio-residual-blockarb">Audio Residual Block(ARB)</h4>
<p>$ARB(x, y, c)$ where $x$ is the <strong>stride size for the frequency dimension</strong>, $y$ is the <strong>stride size for the temporal dimension</strong>, and $c$ is the <strong>number of output channels</strong>.</p>
<p>$K_1(z)$,$K_2(z)$,and $P(z)$ for ARBs, where $z$ is the <strong>stride size</strong>, are defined as</p>
<!-- raw HTML omitted -->
<ul>
<li>Global pooling:  use a sum of average and max pooling to combine their advantages.</li>
<li>fully connected layers, FC1 and FC2
<ul>
<li><strong>softmax for sound classification</strong> tasks</li>
<li><strong>sigmoid for audio tagging</strong> tasks at the end to obtain predictions.</li>
</ul>
</li>
</ul>
<h5 id="leaky-relu">Leaky ReLU</h5>
<p><strong>Leaky ReLU</strong> with parameter 0.01 is applied as a replacement for ReLU because the use of ReLU can lead to the <strong>dying ReLU problem</strong>.</p>
<!-- raw HTML omitted -->
<h2 id="experiment">Experiment</h2>
<ul>
<li>Adam optimizer , cross-entropy loss, and a mini-batch size of 32.</li>
<li>For the AudioSet dataset,use <strong>One-cycle learning rate policy</strong> with a maximum <strong>learning rate of 0.001</strong>.</li>
<li>For ESC50, UrbanSound8K, and RAVDESS, we use a constant <strong>learning rate of 0.0002 for training from scratch</strong> and a constant <strong>learning rate of 0.0001 for fine-tuning.</strong></li>
</ul>
<h4 id="audioset">AudioSet</h4>
<ul>
<li>A large-scale audio dataset. The dataset includes over 2 million audio recordings with 527 sound classes and the duration of most audio clips is <strong>10 seconds.</strong></li>
<li>A <strong>multilabel dataset</strong> (tagging task). <strong>Sigmoid</strong> is used to obtain predictions of models.</li>
<li>Use a balanced sampling strategy from <em>PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition</em>.</li>
</ul>
<h5 id="audioset-impact-of-data-augmentation-techniques">AudioSet: Impact of Data Augmentation Techniques</h5>
<p>Each system with fixed values of hyperparameters is abbreviated as <strong>ERANN-sm-W</strong>, where sm is the <strong>decreasing temporal size parameter</strong> and <strong>W is the widening factor</strong>.<br>
mixup type:</p>
<ul>
<li>1 training without mixup</li>
<li>2 standard mixup on the waveform</li>
<li>3 modified mixup on the waveform</li>
<li>4 modified mixup on the log mel spectrogram</li>
</ul>
<p>$t_c$ is duration of cropped sections of training audio.</p>
<!-- raw HTML omitted -->
<h5 id="audioset-impact-of-hyperparameters">AudioSet: Impact of Hyperparameters</h5>
<p>Each system with fixed values of hyperparameters is abbreviated as <strong>ERANN-sm-W</strong>, where sm is the <strong>decreasing temporal size parameter</strong> and <strong>W is the widening factor</strong>.</p>
<!-- raw HTML omitted -->
<p>Increasing $s_m$ reduces FLOPs and increases the inference speed of models because tensors between convolutional layers have fewer sizes.</p>
<h5 id="audioset-result">AudioSet: result</h5>
<p>Each system with fixed values of hyperparameters is abbreviated as <strong>ERANN-sm-W</strong>, where sm is the <strong>decreasing temporal size parameter</strong> and <strong>W is the widening factor</strong>.</p>
<!-- raw HTML omitted -->
<p>AST was pre-trained on ImageNet , ERANN-5 system is trained from scratch.</p>
<h4 id="esc-50">ESC-50</h4>
<p>Environmental Sound Classification, comprising 50 sound classes and 2000 5-second audio recordings. This dataset is balanced with 40 audio recordings per sound class.</p>
<h5 id="esc-50-impact-of-hyperparameters-and-audioset-pre-training">ESC-50: Impact of Hyperparameters and AudioSet Pre-training</h5>
<p>Each system with fixed values of hyperparameters is abbreviated as <strong>ERANN-sm-W</strong>, where sm is the <strong>decreasing temporal size parameter</strong> and <strong>W is the widening factor</strong>.</p>
<!-- raw HTML omitted -->
<h4 id="urbansound8k">UrbanSound8K</h4>
<p>The UrbanSound8K dataset includes <strong>8,732 audio clips</strong> with <strong>10 classes</strong> of urban sounds. Audio signals have various durations, which do <strong>not exceed 4 seconds.</strong></p>
<h5 id="urbansound8k-results">UrbanSound8K Results</h5>
<p>Each system with fixed values of hyperparameters is abbreviated as <strong>ERANN-sm-W</strong>, where sm is the <strong>decreasing temporal size parameter</strong> and <strong>W is the widening factor</strong>.</p>
<p>Not employ the <strong>temporal cropping</strong> because the duration of audio signals significantly varies ( <strong>from 0.05 to 4 seconds</strong> )</p>
<!-- raw HTML omitted -->
<h4 id="ravdess">RAVDESS</h4>
<p>The RAVDESS dataset includes <strong>speech and song recordings</strong> of <strong>24 professional</strong> actors with <strong>eight diverse emotions.</strong><br>
We use the speech set that comprises <strong>1440 audio recordings</strong> with an average duration of <strong>4 seconds.</strong></p>
<p>Augmentation: for RAVDESS, we use <strong>modified mixup on the waveform</strong>, the <strong>temporal cropping</strong> (tc = 3), and <strong>SpecAugment</strong> during training. We also use <strong>pitch shifting(音高移位).</strong></p>
<!-- raw HTML omitted -->
<p>Pre-training on the AudioSet dataset does not improve the performance of systems on RAVDESS.<br>
ERANNs with $s_m$ &gt; 0 have worse performance than ERANNs with $s_m$ = 0.</p>
<h2 id="conclusion">Conclusion</h2>
<ul class="pa0">
  
</ul>
<div class="mt6 instapaper_ignoref">
      
      
      </div>
    </div>

    <aside class="w-30-l mt6-l">




</aside>

  </article>

    </main>
    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="http://siuoly.github.io" >
    &copy;  Siuoly's site 2022 
  </a>
    <div>
<div class="ananke-socials">
  
</div></div>
  </div>
</footer>

  </body>
</html>
